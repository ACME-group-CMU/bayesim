{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ideal Diode Example\n",
    "\n",
    "## Introduction\n",
    "This is meant to be a (relatively) self-contained example of using `bayesim` to perform a fit - in this case, fitting solar cell outputs to a simple diode model. In this case, we use the same model function to generate our \"observed\" data as the one we use to fit; this allows us to confirm that `bayesim` can retrieve the same input parameters and get the right answer under a very controlled setting.\n",
    "\n",
    "The overall procedure is:\n",
    "1. Define parameters to be fit (starting on a 20 x 20 grid) and initialize a uniform prior probability distribution\n",
    "2. Generate the \"observed\" data and import it (there will be 59 different observation conditions)\n",
    "3. Generate the modeled data to compare to observations - this will be done at every set of experimental conditions that were observed and at every point in the space defined by the fitting parameters (20 x 20 x 59 model computations)\n",
    "4. Use Bayesian inference to compute the posterior probability distribution over the fitting parameters.\n",
    "5. Choose a region of the fitting parameter space exceeding some threshold probability density and resample this region more densely (subdivide each grid box), discarding points outside the region.\n",
    "6. Compute the new modeled data over this new fitting parameter space.\n",
    "7. Repeat steps 4-6 until desired fit precision is reached.\n",
    "\n",
    "## How to use this notebook\n",
    "In many of the code cells, I've included commented-out lines representing alternative options to try. I suggest you step through the whole notebook once (evaluate each cell using <kbd>Shift</kbd>+<kbd>Enter</kbd>) without changing these to understand what's going on, then go back to the beginning and start tweaking them to see how the different options impact the progress and results of the fitting process.\n",
    "\n",
    "## The ideal diode model\n",
    "\n",
    "The simplest way to model a solar cell (current density $J$ as a function of voltage $V$ and temperature $T$) is as an **ideal diode**, described by the following equation:\n",
    "$$J(V,T) = J_L+J_0(\\exp{\\frac{qV}{nkT}}-1)$$\n",
    "where $k=8.61733\\times 10^{-5}$ eV/K is Boltzmann's constant, by convention $J_L$ (the light current) is negative and $J_0$ (the saturation current) is positive but strongly dependent on temperature, a dependence we can approximate as:\n",
    "$$J_0 \\approx B'T^{3/n}\\exp{\\frac{-E_{g0}}{nkT}}$$\n",
    "We'll use the silicon zero-temperature bandgap, $E_{g0}=1.2$ eV, meaning we just need to fit for $B'$ in this equation.\n",
    "$J_L$ is typically directly proportional to light intensity, which we will treat as fixed for this example, leaving us with two parameters to fit for: $B'$ and the ideality factor $n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some code that we'll need\n",
    "import numpy as np\n",
    "import sys\n",
    "import math\n",
    "import pandas as pd\n",
    "import bayesim.model as bym\n",
    "import bayesim.params as byp\n",
    "import deepdish as dd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. List the parameters\n",
    "First, we list our fitting parameters, specifying their names, ranges of values, spacing, lengths, and units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl = byp.Param_list()\n",
    "pl.add_fit_param(name='Bp', display_name=\"B'\", val_range=[10,1000], spacing='log', length=20, units='arb.')\n",
    "pl.add_fit_param(name='n', val_range=[1,2], length=20, min_width=0.01) # units default to \"unitless\"\n",
    "\n",
    "# --------- alternative options --------\n",
    "# try different val_range or length or play with min_width to see what it does with subdivisions later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can inspect the `Param_list` object and see what kind of information is inside..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that there are both `edges` and `vals` computed - the `vals` will be the values at which computations will be done, while `edges` will be the edges of the grid, used in subdivision steps later on.\n",
    "\n",
    "Finally, we define the experimental conditions (EC's for short) and output variable. We won't be varying light intensity here, so we only need voltage and temperature. We also set voltage as the parameter to be plotted on the x-axis when visualizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.add_ec(name='V', units='V', is_x=True) # this will be on the x-axis when we plot data\n",
    "pl.add_ec(name='T', units='K', tolerance=0.1) # tolerance is how precisely we care about knowing this variable's value\n",
    "pl.add_output(name='J', units='A/cm^2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the model\n",
    "Now we're ready to initialize our bayesim Model object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = bym.Model(params=pl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, a uniform probability distribution is created..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.probs.points.head(10) # head(10) just shows the first ten lines in the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Those `min` and `max` values are describing the edges of the grid boxes, and the parameter values are the centers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attach experimental data\n",
    "This is the next step that needs to be done before adding the model as it will use the experimental conditions we enumerate here to compute simulated outputs. For our \"experimental\" data here, we'll actually just simulate some ideal diodes so that we can directly see how good the Bayesian approach is at recovering true underlying parameters.\n",
    "\n",
    "We'll use $n=1.36$ and $B'=258$ as our \"true\" parameters.\n",
    "\n",
    "Here we generate and save the synthetic data as well as plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement the diode model in a function we can call\n",
    "def compute_ID(ec,params):\n",
    "    J_L = -0.03 # treat as fixed for now\n",
    "    V = ec['V']\n",
    "    T = ec['T']\n",
    "    V_th = 8.61733e-5 * T # thermal voltage, kT/q\n",
    "    n = params['n']\n",
    "    Bp = params['Bp']\n",
    "    J0 = Bp*T**(3/n)*math.exp(-1.2/V_th/n)\n",
    "    return J_L + J0*(math.exp(V/V_th/n)-1)\n",
    "\n",
    "# define the vectors of experimental conditions\n",
    "V = np.arange(0,1.0,0.01)\n",
    "T = [float(t) for t in np.arange(150,301,50)]\n",
    "\n",
    "# set the \"true\" values for the parameters\n",
    "params = {'n':1.36,'Bp':258}\n",
    "\n",
    "# generate the \"observations\"\n",
    "data = []\n",
    "for t in T:\n",
    "    J0 = compute_ID({'V':V[0],'T':t},params)\n",
    "    for v in V:\n",
    "        J = compute_ID({'V':v,'T':t},params)\n",
    "        if abs(J)>0.1:\n",
    "            err = 0.05*abs(J)\n",
    "        else:\n",
    "            err = 0.005\n",
    "        # check for \"compliance\"\n",
    "        if not J > 2.0:\n",
    "            data.append([v,t,J,err])\n",
    "\n",
    "# save them to HDF5\n",
    "data = pd.DataFrame.from_records(data=data,columns=['V','T','J','uncertainty'])\n",
    "dd.io.save('obs_data.h5',data)\n",
    "\n",
    "# these next few lines are for generating the plot\n",
    "leg = []\n",
    "for i in range(len(T)):\n",
    "    curve = data[data['T']==T[i]]\n",
    "    plt.plot(curve['V'],curve['J'])\n",
    "    leg.append('%d K'%curve['T'].iloc[0])\n",
    "plt.legend([str(t) for t in leg])\n",
    "plt.xlabel('Voltage [V]')\n",
    "plt.ylabel('Current')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many data points we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated our \"observations,\" we can attach them to `bayesim`. There are some options when we attach the observations, notably `keep_all` and `max_ec_x_step`. If `keep_all` is set to False, `attach_observations` will only import data points that differ from each other by more than 1% of the full y-axis range (this 1% threshold can be modified with the `thresh_dif_frac` option) in the output variable (J) along the defined x-axis variable (V). However, `max_ec_x_step` tells it to only skip a maximum of 0.2 V before saving a point even if it differs by $<1$% from the previous one.\n",
    "\n",
    "This allows us to sample more densely in regions where the output variable is changing rapidly and save simulation load in regions where it changes more slowly.\n",
    "\n",
    "Finally, we define the experimental uncertainty. We could define custom uncertainty values at each measurement point by including an 'uncertainty' column in the file with the observed data. If you would rather use a fixed value of experimental uncertainty for all measurements, use the `fixed_unc` keyword as we do here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.attach_observations(obs_data_path='obs_data.h5', keep_all=False, max_ec_x_step=0.2, fixed_unc=0.0001)\n",
    "\n",
    "# ---------- alternative options ------------\n",
    "\n",
    "# don't discard any data\n",
    "#m.attach_observations(obs_data_path='obs_data.h5', fixed_unc=0.0001)\n",
    "\n",
    "# changing max_ec_x_step\n",
    "#m.attach_observations(obs_data_path='obs_data.h5', keep_all=False, max_ec_x_step=0.1, fixed_unc=0.0001)\n",
    "\n",
    "# changing thresh_dif_frac\n",
    "#m.attach_observations(obs_data_path='obs_data.h5', keep_all=False, max_ec_x_step=0.1, thresh_dif_frac=0.001, fixed_unc=0.0001)\n",
    "\n",
    "# using uncertainty values from file rather than fixed value\n",
    "#m.attach_observations(obs_data_path='obs_data.h5', keep_all=False, max_ec_x_step=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many data points of those 305 from before were kept?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(m.obs_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the data looks like once we've trimmed and imported it - we'll show both the full current range as well as a zoomed-in one to see the impact of discarding some data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code for plotting, not critical to understand\n",
    "fig, ax = plt.subplots(1,2,figsize=(11,5))\n",
    "leg = []\n",
    "plt.subplot(1,2,1)\n",
    "for i in range(len(T)):\n",
    "    curve = m.obs_data[m.obs_data['T']==T[i]]\n",
    "    plt.plot(curve['V'],curve['J'])\n",
    "    leg.append('%d K'%curve['T'].iloc[0])\n",
    "plt.legend([str(t) for t in leg])\n",
    "plt.xlabel('Voltage [V]')\n",
    "plt.ylabel('Current')\n",
    "plt.subplot(1,2,2)\n",
    "for i in range(len(T)):\n",
    "    curve = m.obs_data[m.obs_data['T']==T[i]]\n",
    "    plt.plot(curve['V'],curve['J'])\n",
    "    leg.append('%d K'%curve['T'].iloc[0])\n",
    "plt.ylim([-0.04,0.1])\n",
    "plt.legend([str(t) for t in leg])\n",
    "plt.xlabel('Voltage [V]')\n",
    "plt.ylabel('Current')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Attach modeled data\n",
    "Now we add the function that gives the modeled data - in this case, the `compute_ID` function we just defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.attach_model(mode='function', model_data_func=compute_ID)\n",
    "\n",
    "# how to attach from a file instead\n",
    "#m.attach_model(mode='file',fpath='model_data.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When that function is called, `bayesim` also generates the full table of modeled data at all experimental conditions in the observed data we attached before. Check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also compute the \"model uncertainty\" - the difference in output variable (at a fixed set of experimental conditions) when one moves around the fitting parameter space. For each point, the likelihood will be calculated with a standard deviation equal to the maximum of the experimental uncertainty and the model uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.calc_model_unc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.model_data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Drop that Bayes!\n",
    "We're ready to do our Bayesian inference! The `run` function has many options we can set but can also be called with no arguments to run in the default way. In this case, the default is:\n",
    "\n",
    "A threshold probability concentration is defined by an amount of probability mass (`th_pm`, defaults to 0.8) and a fraction of the fitting parameter space volume (`th_pv`, defaults to 0.05). `bayesim` chooses observation points randomly (without replacement) for Bayesian updates until at least `th_pm` of the probability resides in at most `th_pv` of the parameter space.\n",
    "\n",
    "However, depending on the thresholds set, data sampling and other factors, this threshold could potentially take very few observation points to reach, meaning we will inevitably be sampling our experimental conditions unevenly. To address this, a parameter `min_num_pts` is defined (with a default value of 0.7 * the number of observed data points). The procedure just described is repeated until that number of points has been used, and the resulting probability distributions from each cycle are averaged together to produce the final result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.run()\n",
    "\n",
    "# ------- alternative options ------\n",
    "\n",
    "# different thresholding conditions\n",
    "#m.run(th_pm=0.9, th_pv=0.02)\n",
    "\n",
    "# use all the observed data\n",
    "#m.run(min_num_pts=len(m.obs_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize the distribution and add markers for the real values to compare..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save_state('after_run_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m.visualize_probs(true_vals=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there are multiple points in the parameter space that give similarly good fits to the data - in particular, we see a tradeoff between the $B'$ and $n$ parameters, where we can get a similar fit quality by increasing $B'$ and decreasing $n$. Comfortingly, the locus of high-probability points generally centers about the \"true\" values.\n",
    "\n",
    "We can also plot the simulated data corresponding to the most probable points (in this case, three of them) in parameter space and compare to the observed data at the same conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.comparison_plot(num_param_pts=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad. We can also of course look at the probabilities directly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.top_probs(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Let's subdivide!\n",
    "Again we'll use the default options, which will subdivide all grid boxes with probability > 0.001 as well as any immediately adjacent boxes. It will discard any boxes with probability < 0.001. This threshold can be modified with the `threshold_prob` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.subdivide()\n",
    "\n",
    "# try a different threshold probability\n",
    "# m.subdivide(threshold_prob = 0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at the grid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.visualize_grid(true_vals=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute and attach new model data\n",
    "We've added new model points to the grid so we have to compute the modeled values and attach that data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sims = dd.io.load('new_sim_points_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for sim in new_sims.iterrows():\n",
    "    v = sim[1]['V']\n",
    "    t = sim[1]['T']\n",
    "    this_pt = [v,t,compute_ID({'V':v,'T':t},sim[1])]\n",
    "    this_pt.extend([sim[1][n] for n in m.fit_param_names()])\n",
    "    data.append(this_pt)\n",
    "columns = ['V','T','J']\n",
    "columns.extend(m.fit_param_names())\n",
    "new_sim_data = pd.DataFrame.from_records(data=data,columns=columns)\n",
    "dd.io.save('new_sim_data_1.h5',new_sim_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc_model_unc=True runs the model uncertainty calculation without having to make a separate function call\n",
    "m.attach_model(mode='file', model_data_path='new_sim_data_1.h5', calc_model_unc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Onward!\n",
    "Let's do it again!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save_state('after_run_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.visualize_probs(true_vals=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the parameter ranges have narrowed slightly..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.top_probs(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a cool trick with the `comparison_plot` function - we can choose more of the other experimental conditions to plot at!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.comparison_plot(num_param_pts=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.subdivide()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.visualize_grid(true_vals=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.probs.points.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sims = dd.io.load('new_sim_points_2.h5')\n",
    "data = []\n",
    "for sim in new_sims.iterrows():\n",
    "    v = sim[1]['V']\n",
    "    t = sim[1]['T']\n",
    "    this_pt = [v,t,compute_ID({'V':v,'T':t},sim[1])]\n",
    "    this_pt.extend([sim[1][n] for n in m.fit_param_names()])\n",
    "    data.append(this_pt)\n",
    "columns = ['V','T','J']\n",
    "columns.extend(m.fit_param_names())\n",
    "new_sim_data = pd.DataFrame.from_records(data=data,columns=columns)\n",
    "dd.io.save('new_sim_data_2.h5',new_sim_data)\n",
    "m.attach_model(mode='file', model_data_path='new_sim_data_2.h5', calc_model_unc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.save_state('after_run_3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "m.visualize_probs(true_vals=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m.top_probs(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.comparison_plot(num_param_pts=3, num_ecs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Et Cetera, et cetera, et cetera...\n",
    "And you could continue to repeat this process! Eventually you'd get to the point where you can't get any more precision out of the fit because you're limited by the experimental uncertainty.\n",
    "\n",
    "For now, let's compare our \"true\" params with the highest-probability ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.top_probs(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Bp_err = abs((params['Bp']-m.top_probs(1)['Bp'].iloc[0])/params['Bp'])\n",
    "n_err = abs((params['n']-m.top_probs(1)['n'].iloc[0])/params['n'])\n",
    "print(\"Error in B' determination: %.2f%% \\nError in n determination: %.2f%%\"%(100*Bp_err, 100*n_err))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too shabby, I'd say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py36]",
   "language": "python",
   "name": "conda-env-py36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
